---
title: "Projet STAT"
author: "You ZUO, Clément MONNOT"
date: "2019/5/26"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

#Statistiques Descriptives
##<-1.a->

```{r message=FALSE,fig.width=5, fig.height=3, fig.align = "center"}
library(ggplot2)
library(ggthemes)
library(plyr)
library(dplyr)
library(readr)
data <- as.data.frame(read_csv("~/Documents/ENSIIE/S2/MST/Files/Data_Binomial_06.csv"))
for (i in 1:length(data[,'sex'])) {
  if(data[i,'sex']==0) data[i,'sex']="homme"
  else data[i,'sex']="femme"
}
data <- filter(data,victory<=100, victory>=0)

ggplot(data,aes(sex,victory,fill=sex,colour = sex)) +
  geom_boxplot(alpha=0.25, outlier.alpha=0) +
  geom_jitter(fill="black", size = 0.1) +
  stat_summary(fun.y=mean, colour="white", geom="point", shape=18, size=1) +
  xlab("Genre") + ylab("Pourcentage de victoires")
```

D'après le graphique ci-dessus, on peut voir que les pourcentages de victoires des hommes et des femmes sont assez proches mais que la quantité de joueurs masculins est plus grande que celle des joueurs féminins.

##<-1.b->

```{r message=TRUE,fig.width=5, fig.height=3, fig.align = "center"}
library(lattice)
for (i in 1:length(data[,'age'])) {
  if(data[i,'age']==0) data[i,'age']="jeune"
  else {if(data[i,'age']==1) data[i,'age']="adulte"
  else data[i,'age']="âgé"}
}
ggplot(data,aes(age,victory,fill=age,colour=age)) +
  geom_boxplot(alpha=0.25, outlier.alpha=0) +
  geom_jitter(fill="black", size = 0.1) +
  stat_summary(fun.y=mean, colour="white", geom="point", shape=18, size=1) +
  xlab("Age") + ylab("Pourcentage de victoires")
```
On observe une moyenne du pourcetage de victoires légèrement plus élevée chez les jeunes malgré une population plus faible par rapport aux adultes et aux personnées âgées.
```{r message=FALSE, warning=TRUE,fig.width=5, fig.height=3, fig.align = "center"}
ggplot(data,aes(cost,victory)) +
  geom_point(alpha = 0.2) +
  geom_smooth() +
  xlab("Prix des jeux") + ylab("Pourcentage de victoires")
```
On peut voir que la pourcentage de victoires augmente de manière, a priori, proportiennelle par rapport aux prix des jeux.

##<-1.c->
```{r ,fig.width=5, fig.height=3, fig.align = "center"}
library(Rmisc)
ggplot(data,aes(cost)) + 
  geom_histogram(bins = 30, color = 'black') +
  xlab("Prix des jeux") 
  
```
Selon le premier graphique, on peut supposer que le prix du jeu suit une loi exponentielle avec deux parametres car la distribution des échantillons montre une décroissance exponentielle à partir d'un certain point strictement positif. 

##<-1.d->
```{r fig.height=3, fig.align = "center"}
p2 <- ggplot(data,aes(victory)) + 
  geom_histogram(bins = 30, color = 'black') +
  xlab("Pourcentage de victoires")
p22 <- ggplot(data, aes(sample=victory)) +
  stat_qq() + stat_qq_line(color = "red")
multiplot(p2,p22,cols = 2)
```

Selon la tendance de sa distribution, on a supposé qu'il obéissait à la distribution normale, mais après le test du quantile-quantile, il constate qu'il n'obéit pas à la distribution normale.

#Estimation

```{r fig.width=5, fig.height=4, fig.align = "center"}
p <- ppoints(200) # 生成100个等距结点   
q <- quantile(data[,'cost'],p=p) #生成样本分布的分位数   
plot(qexp(p),q, main="Exponential Q-Q Plot", xlab="Theoretical Quantiles",ylab="Sample Quantiles")    
qqline(q, distribution=qexp,col="red", lty=2)
```

Afin de vérifier si l'échantillon obéit à la distribution exponentielle, on a effectué un test quantile-quantile. Comme le montre la figure ci-dessus, à l'exception des derniers points qui s'écartent de la ligne standard, les autres valeurs sont une bonne indication que notre échantillon obéit à la distribution exponentielle.

##<-2.b->
Pour les échantillons $(X_1,...,X_n)$ qui suivent une loi exponentielle la vraisemblance du modèle est
$$\mathcal{L}(\lambda,x_0|x_1,...,x_n)=\prod_{i=1}^nf(x_n)=\prod_{i=1}^n\lambda\ e^{-\lambda(x_i-x_0)}1_{[x_0,+\infty[}(x_i) $$
$$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ =\lambda^n e^
{-\lambda\sum_{i=1}^n(x_i-x_0)}\prod_{i=1}^n1_{[x_0,+\infty[}(x_i)$$
la log-vraisemblance du modèle est
$$log\mathcal{L}(\lambda,x_0|x_1,...,x_n)=nlog(\lambda )-\lambda
\sum_{i=1}^n(x_i-x_0)+\sum_{i=1}^nlog(1_{[x_0,+\infty[}(x_i))$$

##<-2.c->
Pour le paramètre $\lambda$, soit 
$$\frac{\partial log\mathcal{L}(\lambda,x_0|x_1,...,x_n)}{\partial\lambda}=0$$
Alors, on a
$$\frac{n}{\lambda}-\sum_{i=1}^n(x_i-x_0)=0$$
donc, finalement, l'estimateur de $\lambda$ est
$$\hat{\lambda}=\frac{n}{\sum_{i=1}^n(x_i-x_0)}$$

Pour le paramètre $x_0$, $\mathcal{L}(\lambda,x_0|x_1,...,x_n)$ est maximum si 
$$\prod_{i=1}^n1_{[x_0,+\infty[}(x_i)=1$$
donc $\forall i=1,...,n$ $x_i>x_0$ d'où l'estimateur de $x_{0}$ :
$$\hat{x}_0=min_i(x_i)$$
On va tracer la log-vraisemblance de l’échantillon en fonction de la valeur du paramètre $\lambda$
```{r fig.width=5, fig.height=3, fig.align = "center"}
logvrsb_lambda <- function(lambda){
  res <- sapply(1:length(data[,'cost']), function(i){
    dexp(data[i,'cost'], rate = lambda, log = TRUE)
  })
  -sum(res) 
}
lambdas <- seq(from = 0.0001, to = 0.05, length.out = 100)
logvrsbs_lambda <- data.frame(lambdas, sapply(lambdas,function(lambda){-logvrsb_lambda(lambda)}))
lam <- 0.01192053
colnames(logvrsbs_lambda) <- c('lambda','log_vraisemblance')
ggplot(logvrsbs_lambda,aes(x=lambda, y=log_vraisemblance)) +
  geom_point() +
  geom_vline(xintercept = lam, color = "red") 
```

##<-2.d->
```{r}
stats4::mle(minuslogl = (logvrsb_lambda), start = list(lambda= 0.0001),method= "L-BFGS-B",lower = 0.0001,upper=0.05)
```


##<-2.e->
On suppose que le pourcentage de victoires d’un joueur suit une loi
Normale $\mathcal{N}(40+\frac{P}{10},\sigma^2)$, où P est le prix de son jeu. Pour les échantillons du pourcentage de victoires d’un joueur $V_1,V_2,...,V_n$ la vraisemblance théorique du modèle du pourcentage de victoires est
$$\mathcal{L}(\sigma|v_1,...,v_n)=\prod_{i=1}^nf(v_n)=\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(v_i-40-\frac{p_i}{10})^2}{2\sigma^2})$$
$$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  =\frac{1}{(2\pi)^{\frac{n}{2}}\sigma^n}exp(-\frac{1}{2\sigma^2}\sum_{i=1}^n(v_i-40-\frac{p_i}{10})^2)$$
la log-vraisemblance est
$$log\mathcal{L}(\sigma|v_1,...,v_n)=-\frac{n}{2}log(2\pi)-\frac{n}{2}log(\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^n(v_i-40-\frac{p_i}{10})^2$$

##<-2.f->
Pour déterminer le maximum de vraisemblance du paramètre $\sigma$, on a
$$\frac{\partial log\mathcal{L}(\sigma|v_1,...,v_n)}{\partial\sigma^2}=0$$
donc on a
$$-\frac{n}{2\sigma^2}+\frac{1}{2\sigma^4}\sum_{i=1}^n(v_i-40-\frac{p_i}{10})^2=0$$
$${\hat{\sigma}}^2=\frac{1}{n}\sum_{i=1}^n(v_i-40-\frac{p_i}{10})^2$$

On va tracer la log-vraisemblance de l’échantillon en fonction de la valeur du paramètre $\sigma$
```{r fig.width=5, fig.height=3, fig.align = "center"}
logvrsb_sigma <- function(sigma){
  res <- sapply(1:length(data[,'victory']), function(i){
    dnorm(data[i,'victory'], mean = 40+(data[i,'cost']/10), sd = sigma, log = TRUE)
  })
  sum(res) 
}  
vari_sigmas <- seq(from = 1, to = 30, by = 0.2)
logvrsbs_sigma <- data.frame(vari_sigmas, sapply(vari_sigmas,function(sigma){logvrsb_sigma(sigma)}))
colnames(logvrsbs_sigma) <- c('sigma','log_vraisemblance')
ggplot(logvrsbs_sigma,aes(sigma,log_vraisemblance)) +
  geom_point() 
```


#Intervalles de confiance
##<-3.a->
A la question précédente, on a vérifié que les échantillons suivent une loi exponentielle $\mathcal{E}(\lambda,x_0)$. Dans ce cas là, on a une statistique $\gamma_n=\lambda n\bar{X}_n$ qui suit une loi $\Gamma(1,n)$ donc selon la fonction de densité de $\gamma(x)$ on peut obtenir

$$P_\lambda([\gamma_{\frac{\alpha}{2}}(n)<\lambda n(\bar{X}_n-x_0)<\gamma_{1-\frac{\alpha}{2}}(n)])\longrightarrow1-\alpha$$
lorsque $n$ tend vers l'infini.

Donc on a l’intervalle de confiance asymptotique pour $\lambda$ qui est
$$[\frac{\gamma_{\frac{\alpha}{2}}(n)}{n(\bar{X}_n-x_0)},\frac{\gamma_{1-\frac{\alpha}{2}}(n)}{n(\bar{X}_n-x_0)}]$$

##<-3.b->
En faisant varier $\alpha \in \{5\%,...,50\%\}$, on obtient

```{r fig.width=5, fig.height=3, fig.align = "center"}
alphas <- seq(from = 0.05, to = 0.5, length.out = 30)
x_0 <- min(data[,'cost'])
n <- length(data[,'cost'])
mean_X <- mean(data[,'cost'])

interval <- function(alpha){
  c(qgamma(p=alpha/2, shape = n)/(n*(mean_X-x_0)) ,qgamma(p=1-alpha/2, shape = n)/(n*(mean_X-x_0)))
}
intervalles <- as.data.frame(sapply(alphas, interval))
intervalles <- rbind(intervalles,alphas)
rownames(intervalles) <- c("Limite_inférieure","Limite_supérieure","alpha")
intervalles <- as.data.frame(t(intervalles)) 
ggplot(intervalles,aes(x=alpha)) +
  geom_point(aes(y=Limite_inférieure)) +
  geom_point(aes(y=Limite_supérieure)) +
  ylab("limite")

```
On remarque que plus $\alpha$ est petit, plus l'intervalle de confiance augmente, c'est-à-dire, plus on a de chance que $\lambda$ soit dans l'intervalle de confiance.

#Tests
##<-4.a->
Soit la moyenne du pourcentage de victoires des hommes $\mu_h$ et en fixant $\alpha=5\%$, on fait donc les hypothèses suivantes
$$\begin{cases}
H_0:\ \mu_h\leq 50\%\\
H_1:\ \mu_h> 50\%\\
\end{cases}$$
On a des hypothèses similaires pour les échantillons féminins.
$$\begin{cases}
H_0:\ \mu_f\leq 50\%\\
H_1:\ \mu_f> 50\%\\
\end{cases}$$
Les échantillons du pourcentage de victoires que nous avons testés auparavant ne suivent pas une distribution normale, on ne peut donc pas utiliser le test de Student. Donc on choisit d'utiliser le test de Wilcoxon.

```{r}
vic_homme <- filter(data, sex == "homme")
vic_femme <- filter(data, sex == "femme")
wilcox.test(vic_homme[,'victory'],
            alternative ="less",
            mu = 50, 
            conf.level = 0.95)
```
D'après le resultat, on peut voir que $p-value<\alpha$, ce qui signifie qu'on peut rejeter l'hypothèse $H_0$.

On fait la même chose pour des échantillons du pourcentage de victoires des femmes
```{r}
wilcox.test(vic_femme[,'victory'],
            alternative ="less",
            mu = 50, 
            conf.level = 0.95)
```
D'après le resultat, on peut voir que $p-value<\alpha$, ce qui signifie qu'on peut rejeter l'hypothèse $H_0$. Donc le pourcentage de victoires obtenue par les femmes et les hommes sont, en moyenne, tous les deux inférieurs à 50%.

##<-4.b->
Nous souhaitons comparer le pourcentage de victoires des femmes avec celui des hommes pour voir si ils ont des différences plus évidentes. 
Donc on propose un test d'indépendance:
Soit$(S,V)$ un couple de v.a dans $\{s_0,s_1\}\times \{v_1,...,v_n\}$, où $S$ signifie le genre et $V$ le pourcentage de victoires. Soit $((S_i;V_i);i\leq i\leq n)$ un n-échantillon de $(S,V)$.
On note $p_{ij}=P_{H_0}(S_1=s_i,V_1=v_j)$, et les marginales
$$p_{i.}=\sum_{j=v_1}^{v_n}p_{ij},\ \ p_{.j}=\sum_{i=s_1}^{s_2}p_{ij}$$
On souhaite vérifier si les variables $S$ et $V$ sont indépendantes. On fait les hypothèses suivantes
$$\begin{cases}
H_0:\ p_{ij}=p_{i.}p_{.j}\\
H_1:\ p_{ij}\ne p_{i.}p_{.j}\\
\end{cases}$$
On note les occurrences
$$N_{ij}=\sum_{k=1}^n1_{S_k=s_i,V_k=v_j};\ N_{i.}=\sum_{k=1}^n1_{S_k=s_i};\ N_{.j}=\sum_{k=1}^n1_{V_k=v_j}$$
La statistique du test est:
$$D_n=\sum_{i=1}^2\sum_{j=1}^nD_{ij}=\sum_{i=1}^2\sum_{j=1}^n\frac{(N_{ij}-n\hat{p}_{i.}\hat{p}_{.j})^2}{n\hat{p}_{i.}\hat{p}_{.j}}=\sum_{i=1}^2\sum_{j=1}^n\frac{(N_{ij}-\frac{N_{i.}N_{.j}}{n})^2}{\frac{N_{i.}N_{.j}}{n}}$$
Sous $H_0$, $D_n$ suit une loi de $\mathcal{X}^2_{(n-1)(2-1)}$, donc on a la zone de rejet
$$\mathcal{W}=\{D>F^{-1}_{\mathcal{X}^2_{(n-1)(2-1)}}(1-\alpha)\}$$

```{r warning=FALSE}
tmp <- with(data,table(victory,sex))
chisq.test(tmp)
```
$p-valeur>\alpha$ donc on ne peut pas rejeter l'hypothèse $H_0$, c'est-à-dire qu'il n'existe pas une grande différence entre le pourcentage de victoires des hommes avec celui des femmes.

##<-4.c->
On suppose que les observations du pourcentage de victoires des jeux chers (dont la valeur est supérieure au quantile 70%) $v_{1_c},...,v_{n_c}$ sont issus d’une distribution normale avec un écart-type $\sigma_0 = 5$. On le vérifie par le test de $\mathcal{X}^2$.
On a les hypothèses
$$\begin{cases}
H_0:\ \sigma^2=\sigma_0^2\\
H_1:\ \sigma^2\ne\sigma_0^2\\
\end{cases}$$
ici, la moyenne $\mu$ est inconnue donc on la remplace par la moyenne empirique $\bar{v}_{n_c}$
Sous l'hypothèse $H_0$, on sait que
$$\mathcal{X}^2=\frac{(n_c-1)S_n^{*2}}{\sigma_0^2}=\frac{\sum_{i=1}^n(v_{n_c}-\bar{v}_{n_c})^2}{\sigma_0^2}$$
la zone de rejet
$$\mathcal{W}=\{(v_{1_c},...,v_{n_c});\mathcal{X}^2\geq\mathcal{X}^2_{1-\frac{\alpha}{2}}(n_c-1)\ ou\ \mathcal{X}^2\leq\mathcal{X}^2_{\frac{\alpha}{2}}(n_c-1)\}$$
En calculant la valeur de notre statistique selon nos observations d'échantillons, on obtient 
$$\mathcal{X}^2=1683.108$$
En prenant $n_c=599$, on obtient le quantile de $\mathcal{X}^2(598)=1883$ qui est $1$, c'est à dire qu'on doit rejeter l'hypothèse à partir de $\alpha=0$.
```{r message=TRUE, warning=TRUE, paged.print=TRUE}
prix_cher <- quantile(data[,'cost'],0.7)
data_cher <- filter(data, cost>prix_cher)
varv <- var(data_cher[,'victory'])
moyenne_v <- mean(data_cher[,'victory'])
valchi2 <- sum((data_cher[,'victory']-moyenne_v)^2)/(5^2)
size <- length(data_cher[,'victory'])
quantil <- pchisq(q=1683.108,df=598)
```



